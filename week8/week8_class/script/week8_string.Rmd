---
title: "Week8_String"
author: "Retno K. Ningrum"
date: "2024-10-15"
output: html_document
---

```{r}
library(here)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(janeaustenr)
```

```{r}
words<-"This is a string"
words

#You can also have several strings in a vector.
words_vector<-c("Apples", "Bananas","Oranges")
words_vector

paste("High temp", "Low pH")
paste("High temp", "Low pH", sep = "-")
paste("High temp", "Low pH")

#Add a dash in between the words
paste("High temp", "Low pH", sep = "-")

#Remove the space in between the words
paste0("High temp", "Low pH")

#Working with vectors
shapes <- c("Square", "Circle", "Triangle")
paste("My favorite shape is a", shapes)
shapes <- c("Square", "Circle", "Triangle")
paste("My favorite shape is a", shapes)
two_cities <- c("best", "worst")
paste("It was the", two_cities, "of times.")

#Let's say you want to know how long a string is:
shapes # vector of shapes
str_length(shapes) # how many letters are in each word?

#want to extract specific characters
seq_data<-c("ATCCCGTC")
str_sub(seq_data, start = 2, end = 4) # extract the 2nd to 4th AA

#modify strings
str_sub(seq_data, start = 3, end = 3) <- "A" # add an A in the 3rd position
seq_data

#duplicate patterns in your string
str_dup(seq_data, times = c(2, 3)) # times is the number of times to duplicate each string
```
### Whitespace
you have a column and you did not copy and paste your treatments like you learned in the first week of class. You now have some words with extra white spaces and R thinks its an entirely new word. Here is how to deal with that...
```{r}

badtreatments<-c("High", " High", "High ", "Low", "Low")
badtreatments

#removes white space
str_trim(badtreatments) # this removes both

#removes from one side or the other
str_trim(badtreatments, side = "left") # this removes left

#the opposite of str_trim is str_pad
str_pad(badtreatments, 5, side = "right") # add a white space to the right side after the 5th character

#add character instead of whitespace
str_pad(badtreatments, 5, side = "right", pad = "1") # add a 1 to the right side after the 5th character

```
### Locale sensitive
```{r}
x<-"I love R!"
str_to_upper(x)

#lowercase
str_to_lower(x)

#capital first letter each word
str_to_title(x)


```
### Pattern Matching
```{r}
data<-c("AAA", "TATA", "CTAG", "GCTT")

# find all the strings with an A
str_view(data, pattern = "A")

#detect a specific pattern
str_detect(data, pattern = "A")
str_detect(data, pattern = "AT")

#locate a pattern
str_locate(data, pattern = "AT")
```
Metacharacters
```{r}
vals<-c("a.b", "b.c","c.d")

#string, pattern, replace
str_replace(vals, "\\.", " ")

#let say we had multiple "." in the character vector
vals<-c("a.b.c", "b.c.d","c.d.e")
#string, pattern, replace
str_replace(vals, "\\.", " ")

#string, pattern, replace
str_replace_all(vals, "\\.", " ")
```
### Sequences
Sequences, as the name suggests refers to the sequences of characters which can match. 
```{r}
val2<-c("test 123", "test 456", "test")
str_subset(val2, "\\d")

#count the number of lowercase vowel in each strings
str_count(val2, "[aeiou]")

# count any digit
str_count(val2, "[0-9]")

#Example: find the phone numbers
strings<-c("550-153-7578",
         "banana",
         "435.114.7586",
         "home: 672-442-6739")

phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"

# Which strings contain phone numbers?
str_detect(strings, phone)

# subset only the strings with phone numbers
test<-str_subset(strings, phone)
test
```

## Think, pair, share
Let's clean it up. Lets replace all the "." with "-" and extract only the numbers (leaving the letters behind). Remove any extra white space. You can use a sequence of pipes.
```{r}
test %>%
  str_replace_all(pattern = "\\.", replacement = "-") %>% # replace periods with -
  str_replace_all(pattern = "[a-zA-Z]|\\:", replacement = "") %>% # remove all the things we don't want
  str_trim() # trim the white space
```

### Tidytext
Package for text mining and making text tidy. This is very helpful for social sciences or anyone that uses survey data. Also, really helpful for text mining abstracts to write a review paper on a topic.

Let's analyze a books by Jane Austen.

The function to get all of the text from all of Jane Austen's books is austen_books()

```{r}
# explore it
head(austen_books())
tail(austen_books())

#clean it up and add a column for line and chapter
original_books <- austen_books() %>% # get all of Jane Austen's books
  group_by(book) %>%
  mutate(line = row_number(), # find every line
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", # count the chapters (starts with the word chapter followed by a digit or roman numeral)
                                                 ignore_case = TRUE)))) %>% #ignore lower or uppercase
  ungroup() # ungroup it so we have a dataframe again
# don't try to view the entire thing... its >73000 lines...
head(original_books)

```

Because we are interest in text mining, we will want to clean this so that there is only one word per row so its tidy. In tidytext each word is refered to as a token. The function to unnest the data so that its only one word per row is unnest_tokens().
```{r}
tidy_books <- original_books %>%
  unnest_tokens(output = word, input = text) # add a column named word, with the input as the text column
head(tidy_books) # there are now >725,000 rows. Don't view the entire thing!

#see an example of all the stopwords
head(get_stopwords())

cleaned_books <- tidy_books %>%
  anti_join(get_stopwords()) # dataframe without the stopwords

head(cleaned_books)

#count the most common words across all her book 
cleaned_books %>%
  count(word, sort = TRUE)
```
### Sentiment analysis
There are many ways that we can now analyze this tidy dataset of text. One example is we could do a sentiment analysis (how many positive and negative words) using get_sentiments(). An important note: I was not an English major and I know there are many different lexicons, but I know nothing about them. Look at the help files if you want to go deeper into this...
```{r}
sent_word_counts <- tidy_books %>%
  inner_join(get_sentiments()) %>% # only keep pos or negative words
  count(word, sentiment, sort = TRUE) # count them
head(sent_word_counts)[1:3,]

#create the plot
sent_word_counts %>%
  filter(n > 150) %>% # take only if there are over 150 instances of it
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>% # add a column where if the word is negative make the count negative
  mutate(word = reorder(word, n)) %>% # sort it so it gows from largest to smallest
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```
### Make a wordcloud
```{r}
words<-cleaned_books %>%
  count(word) %>% # count all the words
  arrange(desc(n))%>% # sort the words
  slice(1:100) #take the top 100
wordcloud2(words, shape = 'triangle', size=0.3) # make a wordcloud out of the top 100 words
```

